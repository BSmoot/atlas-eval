# Cornerstone 02: Program Architecture 2026

## Five Programs, One System

The 2026 work is organized into five interconnected programs. They are not parallel tracks—they form a system where each program enables and depends on others.

---

## Program Overview

| # | Program | Core Question | Owner Domain |
|---|---------|---------------|--------------|
| 01 | **Intelligent Organic Growth** | Can we prove repeatable, predictable sales mechanics? | Sales |
| 02 | **Sales Enablement & Demand Gen** | Can we accelerate rep productivity and reduce ramp time? | Marketing / Enablement |
| 03 | **Standardized, Tech-Enabled Experience** | Can we make CX measurable, scalable, and tech-enabled? | Customer Experience |
| 04 | **Optimized Foundations** | Can we enable data-driven decisions through governance and trusted data? | Data / Technology |
| 05 | **Alignment & Accountability** | Can we transform decision-making through a single prioritization framework? | Operations / Culture |

A sixth program slot is reserved for cross-functional initiatives that emerge from The Gateway.

---

## Program 01: Intelligent Organic Growth

**Purpose:** Prove that sales mechanics are repeatable and predictable—not dependent on individual heroics.

**Key Targets:**
- 20% of new revenue from new logos
- 15% direct sales revenue growth
- 3x pipeline coverage to MRR targets
- 33%+ CX MRR booked vs 2025
- 80% of sales team above quota

**Quarterly Arc:**
- Q1: Tool selection, forecasting conversion (MRC→MRR), ramp inspection
- Q2: Industry playbooks, BDR/seller alignment, intent data integration
- Q3: Full deployment, conversion analytics, coordinated buyer journeys
- Q4: Validation—repeatable motions, predictable results, scalable model

**Critical Dependencies:**
- Program 02 for enablement infrastructure
- Program 04 for analytics and data quality
- AUX marketing deliverables (no internal marketing org yet)

---

## Program 02: Sales Enablement & Demand Gen

**Purpose:** Build infrastructure that reduces time-to-first-deal and makes rep productivity measurable.

**Key Deliverables:**
- Marketing automation platform operational
- Searchable content library with usage tracking
- Win/loss intelligence program
- Playbooks by vertical/category/scenario
- Experimentation framework with decision criteria

**Quarterly Arc:**
- Q1: Tool infrastructure (Gong, marketing automation), baseline diagnostics
- Q2: Content library, win/loss program, cross-team feedback loops
- Q3: Playbooks, buyer journey mapping, testing cadence
- Q4: Prove repeatability, accurate forecasting, capacity planning

**Open Questions Requiring Resolution:**
- Is cross-sell a distinct motion with its own targets and ownership?
- Does lifetime MRR payout dilute hunger for new wins?
- Where can technology/data be sold as differentiated value-add?

---

## Program 03: Standardized, Tech-Enabled Experience

**Purpose:** Make customer engagement measurable and scalable through standards and technology.

**Key Deliverables:**
- Lumopath deployed across all CX with role-specific dashboards
- Four quality metrics operational (engagement, health, renewal, CSAT)
- CX Playbooks V2.0 published with governance
- 120+ Portal activations
- AI augmentation ownership framework

**Quarterly Arc:**
- Q1: Lumopath rollout, artifact inventory, metric definitions
- Q2: Daily workflow embedding, automated metrics, Portal integration
- Q3: Playbooks published, AI assessment, quality targets validated
- Q4: Full-year analysis, AI pilots, 2027 roadmap

**Critical Dependencies:**
- Program 04 data domain definitions (semantic consistency for AI)
- Program 02 sales enablement documentation (artifact integration)
- Product team capacity for Lumopath development

---

## Program 04: Optimized Foundations

**Purpose:** Enable data-driven decisions through mature governance, reliable measurement, and trusted reporting.

**Key Deliverables:**
- Data Governance program with domain reviews
- Data Catalog and Semantic Layer
- Self-service analytics via data marts
- Data Governance Rules Engine with scoring
- Visualization MCP for AI-assisted reporting

**Quarterly Arc:**
- Q1: Governance launch with domain reviews, centralized report team
- Q2: Data Catalog/Semantic Layer build, governance engine prep
- Q3: Weighted scoring reports, AI enablement (Claude MCP)
- Q4: Broad availability, trending reports, quality improvement

**Critical Dependencies:**
- Budget approval (especially for external governance partner)
- Business capacity to actually address surfaced data issues
- Single report builder capacity in Q1-Q2

---

## Program 05: Alignment & Accountability

**Purpose:** Transform decision-making and execution through unified prioritization and connected culture.

**Key Deliverables:**
- Gateway 2.0 operational as prioritization framework
- UPSTACK Story messaging embedded across touchpoints
- Recognition program ("Spotlight") reinforcing behaviors
- Pilot programs: Behind the Scenes, Hindsight, Inside UPSTACK podcast
- Strategy Days format established

**Quarterly Arc:**
- Q1: Gateway 2.0 test, culture calendar, How We Work documentation
- Q2: Recognition program, UPSTACK Huddles, Employer Branding V1
- Q3: Podcast pilot, Intro Days, Strategy Days inaugural
- Q4: Year-in-review, Great Place to Work prep, Values Refresh brief

**Success Criteria:**
- 100% of requests evaluated via The Gateway
- 95% of UPSTACKers can articulate the UPSTACK Story
- Experience Survey scores for Connection and Transparency above average

---

## Program Interdependencies

```
┌─────────────────────────────────────────────────────────────────┐
│                    PROGRAM 05: Alignment                        │
│              (Prioritization, Communication, Culture)           │
│                         The Gateway                             │
└─────────────────────────────────────────────────────────────────┘
                              ▲
                              │ Governs all cross-program prioritization
                              │
┌──────────────────┐    ┌─────┴────────┐    ┌──────────────────┐
│   PROGRAM 01     │◄───│  PROGRAM 02  │───►│   PROGRAM 03     │
│ Intelligent      │    │   Sales      │    │  Standardized    │
│ Organic Growth   │    │  Enablement  │    │   Experience     │
│                  │    │              │    │                  │
│ (Sales execution)│    │(Infrastructure│   │(CX operations)   │
└────────┬─────────┘    └──────────────┘    └────────┬─────────┘
         │                                           │
         │         Buyer journey handoffs            │
         └──────────────────┬────────────────────────┘
                            │
                            ▼
              ┌─────────────────────────┐
              │      PROGRAM 04         │
              │  Optimized Foundations  │
              │                         │
              │ (Data, Governance, AI)  │
              └─────────────────────────┘
                            │
                            ▼
              Data quality enables all measurement
              Semantic layer enables AI augmentation
```

**Key Dependency Chains:**

1. **Program 04 → Programs 01, 02, 03:** Data governance and semantic definitions are prerequisites for reliable metrics, AI features, and analytics across all revenue and CX programs.

2. **Program 02 → Program 01:** Sales enablement infrastructure (playbooks, content, tools) is what Program 01 uses to prove repeatability.

3. **Program 03 ← Program 02:** CX playbooks and artifacts need integration with sales enablement documentation.

4. **Program 05 → All:** The Gateway is the prioritization mechanism for cross-program resource conflicts.

---

## Cross-Program Risk Themes

| Risk | Programs Affected | Mitigation |
|------|-------------------|------------|
| Data quality insufficient for analytics | 01, 02, 03, 04 | Early Program 04 governance, data entry enforcement |
| Tool adoption resistance | 01, 02, 03 | Internal champions, usage metrics, manager accountability |
| Cross-functional coordination failures | All | Named owners per process, escalation paths, Gateway governance |
| Capacity constraints | All | Strict prioritization, Gateway enforcement |
| AI ownership undefined | 03, 04 | Early framework definition before broad assessment |

---

## How Programs Should Interact

1. **Dependencies are explicit.** If your program needs something from another, it's documented and tracked.

2. **The Gateway resolves conflicts.** When programs compete for the same resource or have conflicting timelines, The Gateway is the venue.

3. **Quarterly handoffs matter.** Many Q2 deliverables become Q3 inputs for other programs. These transitions need active management.

4. **Data is foundational.** Program 04's semantic definitions affect what's measurable in Programs 01, 02, and 03. Delays cascade.
