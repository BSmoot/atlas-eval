# Cornerstone 03: Operating Principles 2026

## The Fundamental Shift

2026 is about proving that *how we work* is as scalable as *what we sell*. These operating principles define expected behavior across the organization.

---

## The Gateway: Unified Prioritization

The Gateway is UPSTACK's operating governance system for cross-functional decision-making and prioritization.

**What The Gateway Does:**
- Evaluates and prioritizes all incremental program and project requests
- Resolves resource conflicts between programs
- Makes prioritization decisions visible and traceable

**How It Works:**
- Requests go through Gateway review before resource commitment
- VP and ELT groups test and refine the process
- Decisions are made in the open, not in side channels

**What It Replaces:**
- Ad-hoc prioritization through relationship leverage
- Reactive decision-making based on loudest voice
- Hidden trade-offs that surface late

**Success Measure:** 100% of incremental requests evaluated and prioritized via The Gateway by EOY 2026.

---

## Cross-Functional Teaming Model

The program structure requires teams to work across functional boundaries. This is the expected model:

### Ownership vs. Support

Every deliverable has:
- **Owner:** Single accountable party. Makes decisions, unblocks, accountable for outcome.
- **Supported By:** Contributing functions. Provide resources, expertise, capacity—but do not own the outcome.

This is explicit in every deliverable table. When ownership is ambiguous, work stalls.

### Handoff Discipline

Critical handoffs happen at:
- Pre-sales to post-sales (Programs 01/02 → Program 03)
- Data capture to analysis (Programs 01/02/03 → Program 04)
- Insight to action (Program 04 → All)

Each handoff needs:
- Documented trigger (when does handoff occur?)
- Receiving owner (who picks it up?)
- Quality criteria (what constitutes a complete handoff?)

### Feedback Loops

Programs 01 and 02 explicitly require "insight-to-action-to-insight" mechanisms:
- Insights surface from data
- Actions are taken based on insights
- Results of actions create new insights
- The loop runs continuously, not quarterly

---

## Data-Driven Decision Making

### The Data Hierarchy

| Level | Description | Source |
|-------|-------------|--------|
| **Verified** | Audited, governed, in trusted reporting | Program 04 data marts, governed Salesforce data |
| **Operational** | Captured consistently, not yet governed | System logs, tool exports, manual tracking |
| **Inferred** | Reasoned from patterns, not directly measured | Analysis, correlation, hypothesis |
| **Unknown** | We don't have this data yet | Identified gaps |

When making decisions, be explicit about which level of data you're using. "Inferred" is not bad—but pretending it's "Verified" creates false confidence.

### What Gets Measured

The programs define specific metrics. Key categories:

**Revenue Metrics:**
- Net Revenue Growth (PF LQA)
- Bookings per month (MRR)
- Pipeline coverage (target: 3x to revenue)
- New logo contribution %
- CX MRR growth

**Operational Metrics:**
- Tool adoption rates
- Data quality scores
- Process compliance
- Ramp time (new hires)

**Experience Metrics:**
- CSAT
- Customer engagement/health scoring
- Internal survey scores (connection, transparency)

### Baseline-Target-Impact

Every program should establish:
1. **Baseline:** Where are we on January 1, 2026?
2. **Target:** Where do we need to be by December 31, 2026?
3. **Impact:** What business outcome does hitting the target enable?

Without baselines, targets are arbitrary. Without impact, targets are busywork.

---

## How Communication Works

### Rhythms

| Cadence | Purpose | Venue |
|---------|---------|-------|
| Ongoing | Issue escalation, dependency unblocking | Teams, direct |
| Weekly | Progress tracking, blocker surfacing | Program-level |
| Quarterly | Milestone review, reprioritization | Strategy Days |
| Annual | Full-year assessment, 2027 planning | Leadership reviews |

### Transparency Requirements

Program 05 establishes communication pilots explicitly because *current transparency is insufficient*:
- "Behind the Scenes" — How decisions get made
- "Hindsight" — What we learned from what didn't work
- "Inside UPSTACK" — Context on company direction

These are not nice-to-haves. They address a diagnosed problem: *siloed understanding creates execution drag*.

### The UPSTACK Story

A core messaging framework is being finalized. The expectation:
- 95% of UPSTACKers can articulate it
- It's embedded in all communications and employee touchpoints
- It explains *why* we're doing what we're doing, not just *what*

---

## Technology Embedment

### The Problem Being Solved

New tools are being deployed across Programs 01, 02, and 03:
- Marketing automation platform
- Revenue intelligence (Gong)
- Commission tooling
- Lumopath (CX operations)
- Portal enhancements

Historical pattern: Tools deploy but adoption lags. Value isn't realized.

### The Change Management Approach

Program 03 is building a reusable change management methodology. Core elements:
- Role-specific validation before broad rollout
- Immediate value demonstration (not deferred)
- Usage metrics with manager visibility
- Embedded in daily workflows, not parallel to them

### AI Augmentation Framework

Programs 03 and 04 are establishing ownership for AI:
- What can agents do? (Workflows, knowledge bases, automation)
- Who owns which AI capabilities?
- How does semantic layer enable AI-assisted reporting?

This is being defined *before* broad AI assessment to avoid fragmented ownership.

---

## Accountability Structure

### Named Owners

Every deliverable in the program plans has a named owner. This is deliberate:
- Accountability is personal, not departmental
- Escalation has a clear target
- Success and failure are attributable

### Recognition Alignment

Program 05 is launching "Spotlight" recognition explicitly tied to:
- Cross-functional collaboration
- Tech adoption
- Insights-driven decisions

Recognition reinforces the behaviors we need. If recognition rewards something else, behavior follows the reward.

### Incentive Questions

Open questions flagged for resolution:
- Does lifetime MRR comp dilute new logo focus?
- Are bonus plans aligned to revenue or EBITDA?
- Do non-sales departments have visibility to revenue performance?

Until these are resolved, some incentive misalignment may persist.

---

## Decision Criteria Framework

Programs 01 and 02 explicitly require an experimentation/testing framework with decision criteria:

> "If we find X in testing, we will do Y."

This replaces:
- Endless pilots without decisions
- Opinion-based reversals
- Post-hoc rationalization

The framework should specify:
- What signal we're testing for
- What threshold triggers action
- What action follows each outcome

---

## How to Use These Principles

When facing a decision or designing work:

1. **Is ownership clear?** One owner, supporters identified.
2. **Is it prioritized through The Gateway?** Or is it a side-channel ask?
3. **What data level supports it?** Verified, operational, inferred, or unknown?
4. **What handoffs are involved?** Are they documented?
5. **What decision criteria apply?** If we see X, we do Y.
6. **Does recognition reinforce it?** Or does the incentive point elsewhere?

If you can't answer these, the initiative isn't ready to execute.
